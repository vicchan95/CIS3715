{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "## More EDA: improving expertise in loading, cleaning, and analyzing data\n",
    "\n",
    "The objective of Lab 3 is for you to become more proficient in obtaining and working with different types of data. A particular emphasis will be on dealing with text data.\n",
    "\n",
    "This lab assignment will have 3 components. \n",
    "\n",
    "## Lab 3.A. Complete tutorials from Harvard's CS109 Lab 1\n",
    "\n",
    "Go to https://github.com/cs109/2015lab1 and download the following files in your local Lab3 directory:\n",
    "- https://github.com/cs109/2015lab1/blob/master/all.csv\n",
    "- https://github.com/cs109/2015lab1/blob/master/hamlet.txt\n",
    "\n",
    "We are going to go through the *Lab1-babypython.ipynb* and *Lab1-pythonpandas.ipynb*. The orginal Python notebooks were written in Python 2. We converted the notebooks into Python 3, which can be downloaded from here\"\n",
    "\n",
    "- https://github.com/CIS3715-temple-2019/CIS3715-temple-2019.github.io/blob/master/CIS3715-Lab3.A-babypython_py3.ipynb\n",
    "- https://github.com/CIS3715-temple-2019/CIS3715-temple-2019.github.io/blob/master/CIS3715-Lab3.A-pythonpandas_py3.ipynb\n",
    "\n",
    "Study all the code and run every block of code from the *babypython* tutorial. It covers many of the things you already learned in your Labs 1 and 2, so it is a good refresher. However, there are some new things. In particular, you will learn how to load a pure textual file and process it to find counts of all the unique words (also called the tokens) in the text.\n",
    "\n",
    "Study all the code and run every block of code from the *pythonpandas* tutorial. Again, you will find there many things you already know. However, the novelty here is in processing and analysis of a slightly messy tabular data than was the case with the *Auto MPG data*.\n",
    "\n",
    "\n",
    "\n",
    "**Deliverable**: submit the two .ipynb files after you have run all the lines of code. We will appreciate if we see that you put some extra effort, such as trying to modify existing code, enter new lines of code, or provide comments in the text. Make sure any modifications are easily visible by us for the grading purposes.\n",
    "\n",
    "## Lab 3.B. Movie Lens Data\n",
    "\n",
    "In this part of the lab, you will be working on an exercise that is a slightly modified and shortened version of https://github.com/cs109/2015/blob/master/Lectures/02-DataScrapingQuizzes.ipynb. In particular, you will learn how to load and analyze MoviLens data, which contains ratings of multiple movies by multiple users.\n",
    "\n",
    "**The MovieLens data**\n",
    "\n",
    "http://grouplens.org/datasets/movielens/\n",
    "\n",
    "Take some time to learn about the data, because it will be helpful to do the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all imports\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4 #this is beautiful soup\n",
    "import time\n",
    "import operator\n",
    "import socket\n",
    "import re # regular expressions\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  age sex  occupation zip_code\n",
       "0        1   24   M  technician    85711\n",
       "1        2   53   F       other    94043\n",
       "2        3   23   M      writer    32067\n",
       "3        4   24   M  technician    43537\n",
       "4        5   33   F       other    15213"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the user data:\n",
    "#   pass in column names for each CSV\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "\n",
    "users = pd.read_csv(\n",
    "    'http://files.grouplens.org/datasets/movielens/ml-100k/u.user', \n",
    "    sep='|', names=u_cols, engine='python')\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp\n",
       "0      196       242       3       881250949\n",
       "1      186       302       3       891717742\n",
       "2       22       377       1       878887116\n",
       "3      244        51       2       880606923\n",
       "4      166       346       1       886397596"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the ratings:\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(\n",
    "    'http://files.grouplens.org/datasets/movielens/ml-100k/u.data', \n",
    "    sep='\\t', names=r_cols, engine='python')\n",
    "\n",
    "ratings.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>imdb_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id              title release_date  video_release_date  \\\n",
       "0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
       "1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
       "2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
       "3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
       "4         5     Copycat (1995)  01-Jan-1995                 NaN   \n",
       "\n",
       "                                            imdb_url  \n",
       "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...  \n",
       "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...  \n",
       "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...  \n",
       "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...  \n",
       "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the movies data\n",
    "#  the movies file contains columns indicating the movie's genres\n",
    "#  let's only load the first five columns of the file with usecols\n",
    "m_cols = ['movie_id', 'title', 'release_date', \n",
    "            'video_release_date', 'imdb_url']\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'http://files.grouplens.org/datasets/movielens/ml-100k/u.item', \n",
    "    sep='|', names=m_cols, usecols=range(5), engine='python', encoding='ISO-8859-1')\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id                int64\n",
      "title                  object\n",
      "release_date           object\n",
      "video_release_date    float64\n",
      "imdb_url               object\n",
      "dtype: object\n",
      "\n",
      "          movie_id  video_release_date\n",
      "count  1682.000000                 0.0\n",
      "mean    841.500000                 NaN\n",
      "std     485.695893                 NaN\n",
      "min       1.000000                 NaN\n",
      "25%     421.250000                 NaN\n",
      "50%     841.500000                 NaN\n",
      "75%    1261.750000                 NaN\n",
      "max    1682.000000                 NaN\n"
     ]
    }
   ],
   "source": [
    "print(movies.dtypes)\n",
    "print()\n",
    "print(movies.describe())\n",
    "# *** Why only those two columns? ***\n",
    "# Describe generates descriptive analysis that summarizes central tendency, dispersion, etc.\n",
    "# This means it needs numerical data values that PEMDAS can apply to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting data:\n",
    "\n",
    "* DataFrame => group of Series with shared index\n",
    "* single DataFrame column => Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age sex  occupation zip_code\n",
      "0        1   24   M  technician    85711\n",
      "1        2   53   F       other    94043\n",
      "2        3   23   M      writer    32067\n",
      "3        4   24   M  technician    43537\n",
      "4        5   33   F       other    15213\n",
      "user_id                4\n",
      "age                   24\n",
      "sex                    M\n",
      "occupation    technician\n",
      "zip_code           43537\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "users.head()\n",
    "users['occupation'].head()\n",
    "## *** Where did the nice design go? ***\n",
    "columns_you_want = ['occupation', 'sex'] \n",
    "users[columns_you_want].head()\n",
    "\n",
    "print(users.head())\n",
    "\n",
    "print(users.iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering data:\n",
    "\n",
    "Select users older than 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>executive</td>\n",
       "      <td>98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>administrator</td>\n",
       "      <td>91344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>M</td>\n",
       "      <td>administrator</td>\n",
       "      <td>05201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  age sex     occupation zip_code\n",
       "1        2   53   F          other    94043\n",
       "4        5   33   F          other    15213\n",
       "5        6   42   M      executive    98101\n",
       "6        7   57   M  administrator    91344\n",
       "7        8   36   M  administrator    05201"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldUsers = users[users.age > 25]\n",
    "oldUsers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: \n",
    "* show users aged 40 and male\n",
    "* show the mean age of female programmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id        int64\n",
       "age            int64\n",
       "sex           object\n",
       "occupation    object\n",
       "zip_code      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id  age sex  occupation zip_code\n",
      "18        19   40   M   librarian    02138\n",
      "82        83   40   M       other    44133\n",
      "115      116   40   M  healthcare    97232\n",
      "199      200   40   M  programmer    93402\n",
      "283      284   40   M   executive    92629\n",
      "289      290   40   M    engineer    93550\n",
      "308      309   40   M   scientist    70802\n",
      "357      358   40   M    educator    10022\n",
      "397      398   40   M       other    60008\n",
      "564      565   40   M     student    55422\n",
      "646      647   40   M    educator    45810\n",
      "791      792   40   M  programmer    12205\n",
      "841      842   40   M      writer    93055\n",
      "917      918   40   M   scientist    70116\n",
      "\n",
      "     user_id  age sex  occupation zip_code\n",
      "291      292   35   F  programmer    94703\n",
      "299      300   26   F  programmer    55106\n",
      "351      352   37   F  programmer    55105\n",
      "403      404   29   F  programmer    55108\n",
      "420      421   38   F  programmer    55105\n",
      "697      698   28   F  programmer    06906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    943.000000\n",
       "mean      34.051962\n",
       "std       12.192740\n",
       "min        7.000000\n",
       "25%       25.000000\n",
       "50%       31.000000\n",
       "75%       43.000000\n",
       "max       73.000000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# users aged 40 AND male\n",
    "# your code here\n",
    "above40 = users[users.age == 40]\n",
    "above40[:]\n",
    "above40M = above40[above40.sex.str.match('M')]\n",
    "print(above40M[:])\n",
    "print()\n",
    "\n",
    "## users who are female and programmers\n",
    "# your code here\n",
    "usersF = users[users.sex.str.match('F')]\n",
    "usersFprog = usersF[usersF.occupation.str.match('programmer')]\n",
    "print(usersFprog[:])\n",
    "\n",
    "## show statistic summary or compute mean\n",
    "# your code here\n",
    "users.age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Diligent Users\n",
    "\n",
    "- split data per user ID\n",
    "- count ratings\n",
    "- combine result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  unix_timestamp\n",
      "0      196       242       3       881250949\n",
      "1      186       302       3       891717742\n",
      "2       22       377       1       878887116\n",
      "3      244        51       2       880606923\n",
      "4      166       346       1       886397596\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         movie_id  rating  unix_timestamp\n",
       "user_id                                  \n",
       "1             272     272             272\n",
       "2              62      62              62\n",
       "3              54      54              54\n",
       "4              24      24              24\n",
       "5             175     175             175"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ratings.head())\n",
    "## split data\n",
    "grouped_data = ratings.groupby('user_id')\n",
    "#grouped_data = ratings['movie_id'].groupby(ratings['user_id'])\n",
    "\n",
    "## count and combine\n",
    "ratings_per_user = grouped_data.count()\n",
    "\n",
    "ratings_per_user.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**:\n",
    "* get the average rating per movie\n",
    "* advanced: get the movie titles with the highest average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id\n",
      "1    3.878319\n",
      "2    3.206107\n",
      "3    3.033333\n",
      "4    3.550239\n",
      "5    3.302326\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "Max rating: 5\n",
      "\n",
      "Good movie ids:\n",
      "7      465\n",
      "11    1014\n",
      "12     222\n",
      "16     387\n",
      "26      95\n",
      "Name: movie_id, dtype: int64\n",
      "\n",
      "Best movie titles\n",
      "0       Toy Story (1995)\n",
      "119     GoldenEye (1995)\n",
      "128    Four Rooms (1995)\n",
      "139    Get Shorty (1995)\n",
      "168       Copycat (1995)\n",
      "Name: title, dtype: object\n",
      "\n",
      "Number of ratings per movie:\n",
      "     movie_id              title  rating_count\n",
      "0           1   Toy Story (1995)           452\n",
      "452         2   GoldenEye (1995)           131\n",
      "583         3  Four Rooms (1995)            90\n",
      "673         4  Get Shorty (1995)           209\n",
      "882         5     Copycat (1995)            86\n"
     ]
    }
   ],
   "source": [
    "## split data\n",
    "# your code here\n",
    "per_movie_rating = ratings.groupby('movie_id')\n",
    "\n",
    "## average and combine\n",
    "# your code here\n",
    "movie_rating_avg = per_movie_rating['rating'].mean()\n",
    "print(movie_rating_avg.head())\n",
    "print()\n",
    "\n",
    "# get the maximum rating\n",
    "# your code here\n",
    "ratings_max = np.max(ratings['rating'])\n",
    "print('Max rating:', ratings_max)\n",
    "print()\n",
    "\n",
    "# get movie ids with that rating\n",
    "# your code here\n",
    "good_movies = ratings[ratings.rating == 5]\n",
    "\n",
    "print(\"Good movie ids:\")\n",
    "#your code here)\n",
    "print(good_movies['movie_id'].head())\n",
    "print()\n",
    "\n",
    "print(\"Best movie titles\")\n",
    "# your code here\n",
    "gm_titles = pd.merge(movies, good_movies, on='movie_id', how='right')\n",
    "unique_gmtitles = gm_titles.drop_duplicates('title')\n",
    "print(unique_gmtitles['title'].head())\n",
    "print()\n",
    "\n",
    "# get number of ratings per movie\n",
    "# your code here\n",
    "movie_ratesgrouped = ratings.join(ratings.groupby('movie_id').count(), on='movie_id', rsuffix='_count')\n",
    "#print(movie_ratesgrouped)\n",
    "r_per_movie = pd.merge(movies, movie_ratesgrouped, on='movie_id', how='inner')\n",
    "r_per_movie = r_per_movie.drop_duplicates('title')\n",
    "\n",
    "print(\"Number of ratings per movie:\")\n",
    "# your code here\n",
    "print(r_per_movie[['movie_id','title','rating_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**:\n",
    "* get the average rating per user\n",
    "* list all occupations and if they are male or female dominant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  rating_mean\n",
      "66567        1     3.610294\n",
      "9201         2     3.709677\n",
      "7110         3     2.796296\n",
      "68273        4     4.333333\n",
      "53760        5     2.874286\n",
      "occupation     sex\n",
      "administrator  F       36\n",
      "               M       43\n",
      "artist         F       13\n",
      "               M       15\n",
      "doctor         M        7\n",
      "educator       F       26\n",
      "               M       69\n",
      "engineer       F        2\n",
      "               M       65\n",
      "entertainment  F        2\n",
      "               M       16\n",
      "executive      F        3\n",
      "               M       29\n",
      "healthcare     F       11\n",
      "               M        5\n",
      "homemaker      F        6\n",
      "               M        1\n",
      "lawyer         F        2\n",
      "               M       10\n",
      "librarian      F       29\n",
      "               M       22\n",
      "marketing      F       10\n",
      "               M       16\n",
      "none           F        4\n",
      "               M        5\n",
      "other          F       36\n",
      "               M       69\n",
      "programmer     F        6\n",
      "               M       60\n",
      "retired        F        1\n",
      "               M       13\n",
      "salesman       F        3\n",
      "               M        9\n",
      "scientist      F        3\n",
      "               M       28\n",
      "student        F       60\n",
      "               M      136\n",
      "technician     F        1\n",
      "               M       26\n",
      "writer         F       19\n",
      "               M       26\n",
      "Name: user_id, dtype: int64\n",
      "number of male users: \n",
      "670\n",
      "number of female users: \n",
      "273\n"
     ]
    }
   ],
   "source": [
    "# get the average rating per user\n",
    "# your code here\n",
    "group_user = ratings.join(ratings.groupby('user_id').mean(), on='user_id', rsuffix='_mean')\n",
    "sorted_user = group_user.sort_values(by='user_id')\n",
    "sorted_unique = sorted_user.drop_duplicates('user_id')\n",
    "print(sorted_unique[['user_id', 'rating_mean']].head())\n",
    "\n",
    "# list all occupations and if they are male or female dominant\n",
    "# your code here\n",
    "gender_dist = users.groupby(['occupation', 'sex']).count()\n",
    "print(gender_dist['user_id'])\n",
    "## I absolutely couldn't figure out a way to traverse through the dataframe and compare the\n",
    "##   count values for if-else statements...\n",
    "\n",
    "print('number of male users: ')\n",
    "print(sum(users['sex'] == 'M'))\n",
    "\n",
    "print('number of female users: ')\n",
    "print(sum(users['sex'] == 'F'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**:\n",
    "- produce a 1-page document that uses a combination of text, tables, and figures that provide some interesting insights about the Movie Lens data. You should feel free to use outside sources to produce the report, as long as you acknowledge your sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     movie_id                          title  rating_count  avg_ratings\n",
      "49         50               Star Wars (1977)         583.0     4.358491\n",
      "257       258                 Contact (1997)         509.0     3.803536\n",
      "99        100                   Fargo (1996)         508.0     4.155512\n",
      "180       181      Return of the Jedi (1983)         507.0     4.007890\n",
      "292       294               Liar Liar (1997)         485.0     3.156701\n",
      "284       286    English Patient, The (1996)         481.0     3.656965\n",
      "286       288                  Scream (1996)         478.0     3.441423\n",
      "0           1               Toy Story (1995)         452.0     3.878319\n",
      "298       300           Air Force One (1997)         431.0     3.631090\n",
      "120       121  Independence Day (ID4) (1996)         429.0     3.438228\n",
      "\n",
      "      movie_id                                              title  \\\n",
      "1282      1293                                    Star Kid (1997)   \n",
      "1112      1122                     They Made Me a Criminal (1939)   \n",
      "1179      1189                                 Prefontaine (1997)   \n",
      "1637      1653  Entertaining Angels: The Dorothy Day Story (1996)   \n",
      "1191      1201         Marlene Dietrich: Shadow and Light (1996)    \n",
      "1588      1599                      Someone Else's America (1995)   \n",
      "1525      1536                               Aiqing wansui (1994)   \n",
      "807        814                      Great Day in Harlem, A (1994)   \n",
      "1456      1467               Saint of Fort Washington, The (1993)   \n",
      "1489      1500                          Santa with Muscles (1996)   \n",
      "\n",
      "      rating_count  avg_ratings  \n",
      "1282           3.0          5.0  \n",
      "1112           1.0          5.0  \n",
      "1179           3.0          5.0  \n",
      "1637           1.0          5.0  \n",
      "1191           1.0          5.0  \n",
      "1588           1.0          5.0  \n",
      "1525           1.0          5.0  \n",
      "807            1.0          5.0  \n",
      "1456           2.0          5.0  \n",
      "1489           2.0          5.0  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>avg_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>408</td>\n",
       "      <td>Close Shave, A (1995)</td>\n",
       "      <td>112.0</td>\n",
       "      <td>4.491071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>318</td>\n",
       "      <td>Schindler's List (1993)</td>\n",
       "      <td>298.0</td>\n",
       "      <td>4.466443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>169</td>\n",
       "      <td>Wrong Trousers, The (1993)</td>\n",
       "      <td>118.0</td>\n",
       "      <td>4.466102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>483</td>\n",
       "      <td>Casablanca (1942)</td>\n",
       "      <td>243.0</td>\n",
       "      <td>4.456790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "      <td>283.0</td>\n",
       "      <td>4.445230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>603</td>\n",
       "      <td>Rear Window (1954)</td>\n",
       "      <td>209.0</td>\n",
       "      <td>4.387560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "      <td>267.0</td>\n",
       "      <td>4.385768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Star Wars (1977)</td>\n",
       "      <td>583.0</td>\n",
       "      <td>4.358491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>12 Angry Men (1957)</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>Citizen Kane (1941)</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4.292929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     movie_id                             title  rating_count  avg_ratings\n",
       "404       408             Close Shave, A (1995)         112.0     4.491071\n",
       "315       318           Schindler's List (1993)         298.0     4.466443\n",
       "168       169        Wrong Trousers, The (1993)         118.0     4.466102\n",
       "479       483                 Casablanca (1942)         243.0     4.456790\n",
       "63         64  Shawshank Redemption, The (1994)         283.0     4.445230\n",
       "598       603                Rear Window (1954)         209.0     4.387560\n",
       "11         12        Usual Suspects, The (1995)         267.0     4.385768\n",
       "49         50                  Star Wars (1977)         583.0     4.358491\n",
       "177       178               12 Angry Men (1957)         125.0     4.344000\n",
       "133       134               Citizen Kane (1941)         198.0     4.292929"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_avg = pd.DataFrame({'movie_id':movie_rating_avg.index, 'avg_ratings':movie_rating_avg.values})\n",
    "##  r_per_movie = pd.merge(movies, movie_ratesgrouped, on='movie_id', how='right')\n",
    "columns_merged = pd.merge(r_per_movie, movies_avg, on='movie_id', how='right')\n",
    "sortByCount = columns_merged[['movie_id','title', 'rating_count','avg_ratings']].sort_values(by='rating_count',ascending=False)\n",
    "print(sortByCount[:10])\n",
    "print()\n",
    "\n",
    "sortByAvg = columns_merged[['movie_id','title', 'rating_count','avg_ratings']].sort_values(by='avg_ratings',ascending=False)\n",
    "print(sortByAvg[:10])\n",
    "print()\n",
    "\n",
    "above100ct = columns_merged[columns_merged['rating_count']>=100]\n",
    "above100ct = above100ct[['movie_id','title', 'rating_count','avg_ratings']].sort_values(by='avg_ratings',ascending=False)\n",
    "above100ct[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.C. HTML Data\n",
    "\n",
    "In this part of the lab, you will be also be working on an exercise that is a slightly modified and shortened version of https://github.com/cs109/2015/blob/master/Lectures/02-DataScrapingQuizzes.ipynb. In particular, you will learn how to load and analyze html data.\n",
    "\n",
    "HTML:\n",
    "* HyperText Markup Language\n",
    "* standard for creating webpages\n",
    "* HTML tags \n",
    "    - have angle brackets\n",
    "    - typically come in pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example for a minimal webpage defined in HTML tags. The root tag is <html> and then you have the <head> tag. This part of the page typically includes the title of the page and might also have other meta information like the author or keywords that are important for search engines. The <body> tag marks the actual content of the page. You can play around with the <h2> tag trying different header levels. They range from 1 to 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "  <head>\n",
       "    <title>This is a title</title>\n",
       "  </head>\n",
       "  <body>\n",
       "    <h2> Test </h2>\n",
       "    <p>Hello world!</p>\n",
       "  </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmlString = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>This is a title</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2> Test </h2>\n",
    "    <p>Hello world!</p>\n",
    "  </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "htmlOutput = HTML(htmlString)\n",
    "htmlOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Tags:\n",
    "\n",
    "* heading\n",
    "`<h1></h1> ... <h6></h6>`\n",
    "\n",
    "* paragraph\n",
    "`<p></p>` \n",
    "\n",
    "* line break\n",
    "`<br>` \n",
    "\n",
    "* link with attribute\n",
    "\n",
    "`<a href=\"http://www.example.com/\">An example link</a>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping with Python:\n",
    "\n",
    "Example of a simple webpage: http://www.crummy.com/software/BeautifulSoup\n",
    "\n",
    "Good news: \n",
    "    - some browsers help\n",
    "    - look for: inspect element\n",
    "    - need only basic html\n",
    "    - try 'Ctrl-Shift I' in Chrome\n",
    "    - try 'Command-Option I' in Safari\n",
    "   \n",
    "Different useful libraries:\n",
    "    - urllib\n",
    "    - beautifulsoup\n",
    "    - pattern\n",
    "    - soupy\n",
    "    - LXML\n",
    "    - ...\n",
    " \n",
    "The following cell just defines a url as a string and then reads the data from that url using the `urllib` library. If you uncomment the print command you see that we got the whole HTML content of the page into the string variable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\n",
      "\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\n",
      "<link rel=\"stylesheet\" type=\"text/css\" href=\"/nb/themes/Default/nb.css\">\n",
      "<meta name=\"Description\" content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\">\n",
      "<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\n",
      "<meta name=\"author\" content=\"Leonard Richardson\">\n",
      "</head>\n",
      "<body bgcolor=\"white\" text=\"black\" link=\"blue\" vlink=\"660066\" alink=\"red\">\n",
      "<img align=\"right\" src=\"10.1.jpg\" width=\"250\"><br />\n",
      "\n",
      "<p>You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.</p>\n",
      "\n",
      "<div align=\"center\">\n",
      "\n",
      "<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n",
      "\n",
      "<p>\"A tremendous boon.\" -- Python411 Podcast</p>\n",
      "\n",
      "<p>[ <a href=\"#Download\">Download</a> | <a\n",
      "href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\n",
      "\n",
      "<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</small></p>\n",
      "\n",
      "\n",
      "\n",
      "</div>\n",
      "\n",
      "<p><i>If you have questions, send them to <a\n",
      "href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\n",
      "group</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\n",
      "\n",
      "<p>Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "<ol>\n",
      "\n",
      "<li>Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "<li>Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "<li>Beautiful Soup sits on top of popular Python parsers like <a\n",
      "href=\"http://lxml.de/\">lxml</a> and <a\n",
      "href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "</ol>\n",
      "\n",
      "<p>Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "<p>Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\n",
      "\n",
      "<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\n",
      "\n",
      "<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\n",
      "4.7.1</a> (January 6, 2019). You can install Beautiful Soup 4 with\n",
      "<code>pip install beautifulsoup4</code>.\n",
      "\n",
      "<p>In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "<code>python-bs4</code> package (for Python 2) or the\n",
      "<code>python3-bs4</code> package (for Python 3). In Fedora it's\n",
      "available as the <code>python-beautifulsoup4</code> package.\n",
      "\n",
      "<p>Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the <code>bs4/</code> directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using <code>2to3</code>.)\n",
      "\n",
      "<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "\n",
      "<h3>Beautiful Soup 3</h3>\n",
      "\n",
      "<p>Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed. <a\n",
      "href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\n",
      "the Beautiful Soup 3 documentation.</a>\n",
      "\n",
      "<p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "\n",
      "<p>The current release of Beautiful Soup 3 is <a\n",
      "href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\n",
      "2012). You can install Beautiful Soup 3 with <code>pip install\n",
      "BeautifulSoup</code>. It's also available as\n",
      "<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\n",
      "<code>python-BeautifulSoup</code> in Fedora.\n",
      "\n",
      "<p>You can also download the tarball and use\n",
      "<code>BeautifulSoup.py</code> in your project directly.\n",
      "\n",
      "\n",
      "<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\n",
      "\n",
      "<p>Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "<ul>\n",
      "\n",
      "<li><a\n",
      " href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n",
      " Type\"</a>, a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "<li>Reddit uses Beautiful Soup to <a\n",
      "href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\n",
      "a page that's been linked to and find a representative image</a>.\n",
      "\n",
      "<li>Alexander Harrowell uses Beautiful Soup to <a\n",
      " href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n",
      " activities</a> of an arms merchant.\n",
      "\n",
      "<li>The developers of Python itself used Beautiful Soup to <a\n",
      "href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\n",
      "bug tracker from Sourceforge to Roundup</a>.\n",
      "\n",
      "<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\n",
      "uses Beautiful Soup to <A\n",
      "href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\n",
      "statewide election results</a>.\n",
      "\n",
      "<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\n",
      "Applications Branch</a> uses Beautiful Soup in <a\n",
      "href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "</ul>\n",
      "\n",
      "<p>If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or <a\n",
      "href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\n",
      "group</a>.\n",
      "\n",
      "<h2>Development</h2>\n",
      "\n",
      "<p>Development happens at <a\n",
      "href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a\n",
      "href=\"https://code.launchpad.net/beautifulsoup/\">get the source\n",
      "code</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\n",
      "bugs</a>.<hr><table><tr><td valign=\"top\">\n",
      "<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"></a></td><td valign=\"top\">Crummy is &copy; 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></span><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></p></td><td valign=top><p><b>Document tree:</b>\n",
      "<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dl>\n",
      "</dl>\n",
      "</dl>\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "<form method=\"get\" action=\"/search/\">\n",
      "        <input type=\"text\" name=\"q\" maxlength=\"255\" value=\"\"></input>\n",
      "        </form>\n",
      "        </td>\n",
      "\n",
      "</tr>\n",
      "\n",
      "</table>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.crummy.com/software/BeautifulSoup'\n",
    "source = requests.get(url).text\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**:\n",
    "\n",
    "* Is the word 'Alice' mentioned on the beautiful soup homepage?\n",
    "* How often does the word 'Soup' occur on the site?\n",
    "    - hint: use `.count()`\n",
    "* At what index occurs the substring 'alien video games' ?\n",
    "    - hint: use `.find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Alice' not on homepage\n",
      "45\n",
      "-1\n",
      "'alien video games' substring is not in homepage\n"
     ]
    }
   ],
   "source": [
    "## is 'Alice' in source?\n",
    "if 'Alice' in source:\n",
    "    print(\"'Alice is on homepage'\")\n",
    "else:\n",
    "    print(\"'Alice' not on homepage\")\n",
    "\n",
    "## count occurences of 'Soup'\n",
    "print(source.count('Soup'))\n",
    "\n",
    "## find index of 'alien video games'\n",
    "i = source.find('alien video games')\n",
    "if i == -1:\n",
    "    print(source.find('alien video games'))\n",
    "    print(\"'alien video games' substring is not in homepage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup**\n",
    "\n",
    "* designed to make your life easier\n",
    "* many good functions for parsing html code\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      "<head>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "<meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      "</head>\n",
      "<body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/><br/>\n",
      "<p>You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.</p>\n",
      "<div align=\"center\">\n",
      "<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n",
      "<p>\"A tremendous boon.\" -- Python411 Podcast</p>\n",
      "<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\n",
      "<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "</small></p><p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\n",
      "</div>\n",
      "<p><i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\n",
      "group</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\n",
      "<p>Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "</p><ol>\n",
      "<li>Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "</li><li>Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "</li><li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "</li></ol>\n",
      "<p>Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "</p><p>Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "</p><p>Interested? <a href=\"bs4/doc/\">Read more.</a>\n",
      "<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\n",
      "</p><p>The current release is <a href=\"bs4/download/\">Beautiful Soup\n",
      "4.7.1</a> (January 6, 2019). You can install Beautiful Soup 4 with\n",
      "<code>pip install beautifulsoup4</code>.\n",
      "\n",
      "</p><p>In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "<code>python-bs4</code> package (for Python 2) or the\n",
      "<code>python3-bs4</code> package (for Python 3). In Fedora it's\n",
      "available as the <code>python-beautifulsoup4</code> package.\n",
      "\n",
      "</p><p>Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the <code>bs4/</code> directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using <code>2to3</code>.)\n",
      "\n",
      "</p><p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "\n",
      "</p><h3>Beautiful Soup 3</h3>\n",
      "<p>Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed. <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here's\n",
      "the Beautiful Soup 3 documentation.</a>\n",
      "</p><p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "\n",
      "</p><p>The current release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\n",
      "2012). You can install Beautiful Soup 3 with <code>pip install\n",
      "BeautifulSoup</code>. It's also available as\n",
      "<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\n",
      "<code>python-BeautifulSoup</code> in Fedora.\n",
      "\n",
      "</p><p>You can also download the tarball and use\n",
      "<code>BeautifulSoup.py</code> in your project directly.\n",
      "\n",
      "\n",
      "<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\n",
      "</p><p>Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "</p><ul>\n",
      "<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\n",
      " Type\"</a>, a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "</li><li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\n",
      "a page that's been linked to and find a representative image</a>.\n",
      "\n",
      "</li><li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\n",
      " activities</a> of an arms merchant.\n",
      "\n",
      "</li><li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\n",
      "bug tracker from Sourceforge to Roundup</a>.\n",
      "\n",
      "</li><li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\n",
      "uses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\n",
      "statewide election results</a>.\n",
      "\n",
      "</li><li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA's Forecast\n",
      "Applications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "</li></ul>\n",
      "<p>If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\n",
      "group</a>.\n",
      "\n",
      "</p><h2>Development</h2>\n",
      "<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\n",
      "code</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\n",
      "bugs</a>.</p><hr/><table><tr><td valign=\"top\">\n",
      "<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td><td valign=\"top\"><p><b>Document tree:</b>\n",
      "</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\n",
      "</dd></dl>\n",
      "</dd></dl>\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "<form action=\"/search/\" method=\"get\">\n",
      "<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "</form>\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get bs4 object\n",
    "soup = bs4.BeautifulSoup(source)\n",
    " \n",
    "## compare the two print statements\n",
    "print(soup)\n",
    "#print(soup.prettify())\n",
    "\n",
    "## show how to find all a tags\n",
    "soup.findAll('a')\n",
    "\n",
    "## ***Why does this not work? ***\n",
    "soup.findAll('Soup')\n",
    "## Doesn't work because findAll traverses through the search tree with the\n",
    "##   first letter of the string it's trying to find being what's given in the parameters.\n",
    "##     There are no html tags that start with 'Soup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\n",
      "bs4/download/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bs4/download/',\n",
       " '#Download',\n",
       " 'bs4/doc/',\n",
       " '#HallOfFame',\n",
       " 'https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'zine/',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'zine/',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://lxml.de/',\n",
       " 'http://code.google.com/p/html5lib/',\n",
       " 'bs4/doc/',\n",
       " None,\n",
       " 'bs4/download/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html',\n",
       " 'download/3.x/BeautifulSoup-3.2.1.tar.gz',\n",
       " None,\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py',\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " 'http://svn.python.org/view/tracker/importer/',\n",
       " 'http://www2.ljworld.com/',\n",
       " 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/',\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " 'http://laps.noaa.gov/topograbber/',\n",
       " 'http://groups.google.com/group/beautifulsoup/',\n",
       " 'https://launchpad.net/beautifulsoup',\n",
       " 'https://code.launchpad.net/beautifulsoup/',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " '/source/software/BeautifulSoup/index.bhtml',\n",
       " '/self/',\n",
       " '/self/contact.html',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get attribute value from an element:\n",
    "## find tag: this only returns the first occurrence, not all tags in the string\n",
    "first_tag = soup.find('a')\n",
    "print(first_tag)\n",
    "\n",
    "## get attribute `href`\n",
    "print(first_tag.get('href'))\n",
    "\n",
    "## get all links in the page\n",
    "link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0aee102cbae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# if it starts with 'http' we are happy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'http'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mexternal_links\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "## filter all external links\n",
    "# create an empty list to collect the valid links\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links\n",
    "# if it starts with 'http' we are happy\n",
    "for l in link_list:\n",
    "    if l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "\n",
    "# this throws an error! It says something about 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# lets investigate. Have a close look at the link_list:\n",
    "link_list\n",
    "\n",
    "# Seems that there are None elements!\n",
    "# Let's verify\n",
    "print(sum([l is None for l in link_list]))\n",
    "\n",
    "# So there are two elements in the list that are None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://lxml.de/',\n",
       " 'http://code.google.com/p/html5lib/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html',\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py',\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " 'http://svn.python.org/view/tracker/importer/',\n",
       " 'http://www2.ljworld.com/',\n",
       " 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/',\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " 'http://laps.noaa.gov/topograbber/',\n",
       " 'http://groups.google.com/group/beautifulsoup/',\n",
       " 'https://launchpad.net/beautifulsoup',\n",
       " 'https://code.launchpad.net/beautifulsoup/',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's filter those objects out in the for loop\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links\n",
    "# if it is not None and starts with 'http' we are happy\n",
    "for l in link_list:\n",
    "    if l is not None and l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "        \n",
    "external_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The above `if` condition works because of lazy evaluation in Python. The `and` statement becomes `False` if the first part is `False`, so there is no need to ever evaluate the second part. Thus a `None` entry in the list gets never asked about its first four characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://lxml.de/',\n",
       " 'http://code.google.com/p/html5lib/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html',\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " 'https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py',\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " 'http://svn.python.org/view/tracker/importer/',\n",
       " 'http://www2.ljworld.com/',\n",
       " 'http://www.b-list.org/weblog/2010/nov/02/news-done-broke/',\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " 'http://laps.noaa.gov/topograbber/',\n",
       " 'http://groups.google.com/group/beautifulsoup/',\n",
       " 'https://launchpad.net/beautifulsoup',\n",
       " 'https://code.launchpad.net/beautifulsoup/',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can put this in a list comprehension as well, it almost reads like \n",
    "# a sentence.\n",
    "\n",
    "[l for l in link_list if l is not None and l.startswith('http')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>This is a title</title></head><body><h3> Test </h3><p>Hello world!</p></body></html>\n",
      "<head><title>This is a title</title></head>\n",
      "<body><h3> Test </h3><p>Hello world!</p></body>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<body><h3> Test </h3><p>Hello world!</p></body>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redifining `s` without any line breaks\n",
    "s = \"\"\"<!DOCTYPE html><html><head><title>This is a title</title></head><body><h3> Test </h3><p>Hello world!</p></body></html>\"\"\"\n",
    "## get bs4 object\n",
    "tree = bs4.BeautifulSoup(s)\n",
    "\n",
    "## get html root node\n",
    "root_node = tree.html\n",
    "print(root_node)\n",
    "\n",
    "## get head from root using contents\n",
    "head = root_node.contents[0]\n",
    "print(head)\n",
    "\n",
    "## get body from root\n",
    "body = root_node.contents[1]\n",
    "print(body)\n",
    "\n",
    "## could directly access body\n",
    "tree.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**:\n",
    "\n",
    "* Find the `h3` tag by parsing the tree starting at `body`\n",
    "* Create a list of all __Hall of Fame__ entries listed on the Beautiful Soup webpage\n",
    "    - hint: it is the only unordered list in the page (tag `ul`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Reddit uses Beautiful Soup to parse\\na page that's been linked to and find a representative image.\\n\\n\", 'Alexander Harrowell uses Beautiful Soup to track the business\\n activities of an arms merchant.\\n\\n', 'The developers of Python itself used Beautiful Soup to migrate the Python\\nbug tracker from Sourceforge to Roundup.\\n\\n', 'The Lawrence Journal-World\\nuses Beautiful Soup to gather\\nstatewide election results.\\n\\n', 'The NOAA\\'s Forecast\\nApplications Branch uses Beautiful Soup in TopoGrabber, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "## get h3 tag from body\n",
    "tree.body.h3\n",
    "\n",
    "## use ul as entry point\n",
    "#soup_root = soup.html\n",
    "soup.body.ul\n",
    "\n",
    "## get hall of fame list from entry point\n",
    "## skip the first entry \n",
    "hof = soup.body.ul\n",
    "hof_entry = hof.contents[2:]\n",
    "#print(hof_entry)\n",
    "\n",
    "## reformat into a list containing strings\n",
    "## it is ok to have a list of lists\n",
    "tmp = [l.get_text() for l in hof_entry]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tmp` now is actually a list of lists containing the hall of fame entries. \n",
    "Here is some advanced Python on how to print really just one entry per list item.\n",
    "\n",
    "The cool things about this are: \n",
    "* The use of `\"\"` to just access the `join` function of strings.\n",
    "* The `join` function itself\n",
    "* that you can actually have two nested for loops in a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit uses Beautiful Soup to parse\n",
      "a page that's been linked to and find a representative image.\n",
      "\n",
      "\n",
      "Alexander Harrowell uses Beautiful Soup to track the business\n",
      " activities of an arms merchant.\n",
      "\n",
      "\n",
      "The developers of Python itself used Beautiful Soup to migrate the Python\n",
      "bug tracker from Sourceforge to Roundup.\n",
      "\n",
      "\n",
      "The Lawrence Journal-World\n",
      "uses Beautiful Soup to gather\n",
      "statewide election results.\n",
      "\n",
      "\n",
      "The NOAA's Forecast\n",
      "Applications Branch uses Beautiful Soup in TopoGrabber, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test =  [\"\".join(str(a) for a in sublist) for sublist in tmp]\n",
    "#print(test)\n",
    "print('\\n'.join(test))\n",
    "#print(''.join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**:\n",
    "- Explain in detail what is Python doing in the previous line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to 7**:\n",
    "The code above is a double for loop to parse through the data in the list of strings assigned to the `tmp` variable. First the outer for loop assigns each string element within the `tmp` list to `sublist` and then the inner for loop goes through each `sublist` joining them together into one string. Basically what it's doing is creating a repeat of of `tmp` but assigning it to variable `test`. I'm assuming this code was run because if I had created `tmp` as a double nested list of string elements then this code would parse the list to create one list of only string elements and **NOT** a list within a list. However, it still worked because the `.join` is a string method. Therefore everything just rendered as the same list. The `print('\\n'.join(test)` then prints out the result of test by adjoining a newline after each full sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**:\n",
    "- Plot a histogram of the count of the 20 most common words in the html file\n",
    "- Plot a histogram of the count of the 20 most common words in the visible part (what is displayed in the browser) of the html file\n",
    "\n",
    "**Deliverable**: For Lab 3.B and 3.C submit a modified version fo this .ipynb file that contains all the answers to the quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<html>\\n<head>\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\\n<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\\n<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\\n<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\\n<meta content=\"Leonard Richardson\" name=\"author\"/>\\n</head>\\n<body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/><br/>\\n<p>You didn\\'t write that awful page. You\\'re just trying to get some\\ndata out of it. Beautiful Soup is here to help. Since 2004, it\\'s been\\nsaving programmers hours or days of work on quick-turnaround\\nscreen scraping projects.</p>\\n<div align=\"center\">\\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\\n<p>\"A tremendous boon.\" -- Python411 Podcast</p>\\n<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\\n<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n</small></p><p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\\n</div>\\n<p><i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\\n<p>Beautiful Soup is a Python library designed for quick turnaround\\nprojects like screen-scraping. Three features make it powerful:\\n\\n</p><ol>\\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn\\'t take\\nmuch code to write an application\\n\\n</li><li>Beautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don\\'t have to think\\nabout encodings, unless the document doesn\\'t specify an encoding and\\nBeautiful Soup can\\'t detect one. Then you just have to specify the\\noriginal encoding.\\n\\n</li><li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n</li></ol>\\n<p>Beautiful Soup parses anything you give it, and does the tree\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\'s\\ngot bold text, then give me that text.\"\\n\\n</p><p>Valuable data that was once locked up in poorly-designed websites\\nis now within your reach. Projects that would have taken hours take\\nonly minutes with Beautiful Soup.\\n\\n</p><p>Interested? <a href=\"bs4/doc/\">Read more.</a>\\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\\n</p><p>The current release is <a href=\"bs4/download/\">Beautiful Soup\\n4.7.1</a> (January 6, 2019). You can install Beautiful Soup 4 with\\n<code>pip install beautifulsoup4</code>.\\n\\n</p><p>In Debian and Ubuntu, Beautiful Soup is available as the\\n<code>python-bs4</code> package (for Python 2) or the\\n<code>python3-bs4</code> package (for Python 3). In Fedora it\\'s\\navailable as the <code>python-beautifulsoup4</code> package.\\n\\n</p><p>Beautiful Soup is licensed under the MIT license, so you can also\\ndownload the tarball, drop the <code>bs4/</code> directory into almost\\nany Python application (or into your library path) and start using it\\nimmediately. (If you want to do this under Python 3, you will need to\\nmanually convert the code using <code>2to3</code>.)\\n\\n</p><p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\\n\\n</p><h3>Beautiful Soup 3</h3>\\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\\nfrom May 2006 to March 2012. It is considered stable, and only\\ncritical security bugs will be fixed. <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\'s\\nthe Beautiful Soup 3 documentation.</a>\\n</p><p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\\nthe same license as Python itself.\\n\\n</p><p>The current release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\\n2012). You can install Beautiful Soup 3 with <code>pip install\\nBeautifulSoup</code>. It\\'s also available as\\n<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\\n<code>python-BeautifulSoup</code> in Fedora.\\n\\n</p><p>You can also download the tarball and use\\n<code>BeautifulSoup.py</code> in your project directly.\\n\\n\\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\\n</p><p>Over the years, Beautiful Soup has been used in hundreds of\\ndifferent projects. There\\'s no way I can list them all, but I want to\\nhighlight a few high-profile projects. Beautiful Soup isn\\'t what makes\\nthese projects interesting, but it did make their completion easier:\\n\\n</p><ul>\\n<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n</li><li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n</li><li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n</li><li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n</li><li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>.\\n\\n</li><li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</li></ul>\\n<p>If you\\'ve used Beautiful Soup in a project you\\'d like me to know\\nabout, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\ngroup</a>.\\n\\n</p><h2>Development</h2>\\n<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>.</p><hr/><table><tr><td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td><td valign=\"top\"><p><b>Document tree:</b>\\n</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>\\n\\n\\nSite Search:\\n\\n<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>\\n</td>\\n</tr>\\n</table>\\n</body>\\n</html>', '<head>\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\\n<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\\n<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\\n<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\\n<meta content=\"Leonard Richardson\" name=\"author\"/>\\n</head>', '<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>', '<title>Beautiful Soup: We called him Tortoise because he taught us.</title>', '<link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>', '<link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>', '<meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>', '<meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>', '<meta content=\"Leonard Richardson\" name=\"author\"/>', '<body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/><br/>\\n<p>You didn\\'t write that awful page. You\\'re just trying to get some\\ndata out of it. Beautiful Soup is here to help. Since 2004, it\\'s been\\nsaving programmers hours or days of work on quick-turnaround\\nscreen scraping projects.</p>\\n<div align=\"center\">\\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\\n<p>\"A tremendous boon.\" -- Python411 Podcast</p>\\n<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\\n<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n</small></p><p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\\n</div>\\n<p><i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\\n<p>Beautiful Soup is a Python library designed for quick turnaround\\nprojects like screen-scraping. Three features make it powerful:\\n\\n</p><ol>\\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn\\'t take\\nmuch code to write an application\\n\\n</li><li>Beautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don\\'t have to think\\nabout encodings, unless the document doesn\\'t specify an encoding and\\nBeautiful Soup can\\'t detect one. Then you just have to specify the\\noriginal encoding.\\n\\n</li><li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n</li></ol>\\n<p>Beautiful Soup parses anything you give it, and does the tree\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\'s\\ngot bold text, then give me that text.\"\\n\\n</p><p>Valuable data that was once locked up in poorly-designed websites\\nis now within your reach. Projects that would have taken hours take\\nonly minutes with Beautiful Soup.\\n\\n</p><p>Interested? <a href=\"bs4/doc/\">Read more.</a>\\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\\n</p><p>The current release is <a href=\"bs4/download/\">Beautiful Soup\\n4.7.1</a> (January 6, 2019). You can install Beautiful Soup 4 with\\n<code>pip install beautifulsoup4</code>.\\n\\n</p><p>In Debian and Ubuntu, Beautiful Soup is available as the\\n<code>python-bs4</code> package (for Python 2) or the\\n<code>python3-bs4</code> package (for Python 3). In Fedora it\\'s\\navailable as the <code>python-beautifulsoup4</code> package.\\n\\n</p><p>Beautiful Soup is licensed under the MIT license, so you can also\\ndownload the tarball, drop the <code>bs4/</code> directory into almost\\nany Python application (or into your library path) and start using it\\nimmediately. (If you want to do this under Python 3, you will need to\\nmanually convert the code using <code>2to3</code>.)\\n\\n</p><p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\\n\\n</p><h3>Beautiful Soup 3</h3>\\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\\nfrom May 2006 to March 2012. It is considered stable, and only\\ncritical security bugs will be fixed. <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\'s\\nthe Beautiful Soup 3 documentation.</a>\\n</p><p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\\nthe same license as Python itself.\\n\\n</p><p>The current release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\\n2012). You can install Beautiful Soup 3 with <code>pip install\\nBeautifulSoup</code>. It\\'s also available as\\n<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\\n<code>python-BeautifulSoup</code> in Fedora.\\n\\n</p><p>You can also download the tarball and use\\n<code>BeautifulSoup.py</code> in your project directly.\\n\\n\\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\\n</p><p>Over the years, Beautiful Soup has been used in hundreds of\\ndifferent projects. There\\'s no way I can list them all, but I want to\\nhighlight a few high-profile projects. Beautiful Soup isn\\'t what makes\\nthese projects interesting, but it did make their completion easier:\\n\\n</p><ul>\\n<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n</li><li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n</li><li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n</li><li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n</li><li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>.\\n\\n</li><li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</li></ul>\\n<p>If you\\'ve used Beautiful Soup in a project you\\'d like me to know\\nabout, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\ngroup</a>.\\n\\n</p><h2>Development</h2>\\n<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>.</p><hr/><table><tr><td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td><td valign=\"top\"><p><b>Document tree:</b>\\n</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>\\n\\n\\nSite Search:\\n\\n<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>\\n</td>\\n</tr>\\n</table>\\n</body>', '<img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>', '<br/>', \"<p>You didn't write that awful page. You're just trying to get some\\ndata out of it. Beautiful Soup is here to help. Since 2004, it's been\\nsaving programmers hours or days of work on quick-turnaround\\nscreen scraping projects.</p>\", '<div align=\"center\">\\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\\n<p>\"A tremendous boon.\" -- Python411 Podcast</p>\\n<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\\n<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n</small></p><p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>\\n</div>', '<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>', '<h1>Beautiful Soup</h1>', '<p>\"A tremendous boon.\" -- Python411 Podcast</p>', '<p>[ <a href=\"#Download\">Download</a> | <a href=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>', '<a href=\"#Download\">Download</a>', '<a href=\"bs4/doc/\">Documentation</a>', '<a href=\"#HallOfFame\">Hall of Fame</a>', '<a href=\"https://code.launchpad.net/beautifulsoup\">Source</a>', '<a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a>', '<a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>', '<a href=\"zine/\">Zine</a>', '<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n</small></p>', '<small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n</small>', '<a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>', '<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</p>', '<a href=\"zine/\"><i>Tool Safety</i></a>', '<i>Tool Safety</i>', '<p><i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>', '<i>If you have questions, send them to <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i>', '<a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>', '<a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>', '<p>Beautiful Soup is a Python library designed for quick turnaround\\nprojects like screen-scraping. Three features make it powerful:\\n\\n</p>', '<ol>\\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn\\'t take\\nmuch code to write an application\\n\\n</li><li>Beautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don\\'t have to think\\nabout encodings, unless the document doesn\\'t specify an encoding and\\nBeautiful Soup can\\'t detect one. Then you just have to specify the\\noriginal encoding.\\n\\n</li><li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n</li></ol>', \"<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn't take\\nmuch code to write an application\\n\\n</li>\", \"<li>Beautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don't have to think\\nabout encodings, unless the document doesn't specify an encoding and\\nBeautiful Soup can't detect one. Then you just have to specify the\\noriginal encoding.\\n\\n</li>\", '<li>Beautiful Soup sits on top of popular Python parsers like <a href=\"http://lxml.de/\">lxml</a> and <a href=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n</li>', '<a href=\"http://lxml.de/\">lxml</a>', '<a href=\"http://code.google.com/p/html5lib/\">html5lib</a>', '<p>Beautiful Soup parses anything you give it, and does the tree\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\'s\\ngot bold text, then give me that text.\"\\n\\n</p>', '<tt>externalLink</tt>', '<p>Valuable data that was once locked up in poorly-designed websites\\nis now within your reach. Projects that would have taken hours take\\nonly minutes with Beautiful Soup.\\n\\n</p>', '<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\\n</p>', '<a href=\"bs4/doc/\">Read more.</a>', '<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>', '<h2>Download Beautiful Soup</h2>', '<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\\n4.7.1</a> (January 6, 2019). You can install Beautiful Soup 4 with\\n<code>pip install beautifulsoup4</code>.\\n\\n</p>', '<a href=\"bs4/download/\">Beautiful Soup\\n4.7.1</a>', '<code>pip install beautifulsoup4</code>', \"<p>In Debian and Ubuntu, Beautiful Soup is available as the\\n<code>python-bs4</code> package (for Python 2) or the\\n<code>python3-bs4</code> package (for Python 3). In Fedora it's\\navailable as the <code>python-beautifulsoup4</code> package.\\n\\n</p>\", '<code>python-bs4</code>', '<code>python3-bs4</code>', '<code>python-beautifulsoup4</code>', '<p>Beautiful Soup is licensed under the MIT license, so you can also\\ndownload the tarball, drop the <code>bs4/</code> directory into almost\\nany Python application (or into your library path) and start using it\\nimmediately. (If you want to do this under Python 3, you will need to\\nmanually convert the code using <code>2to3</code>.)\\n\\n</p>', '<code>bs4/</code>', '<code>2to3</code>', '<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\\n\\n</p>', '<h3>Beautiful Soup 3</h3>', '<p>Beautiful Soup 3 was the official release line of Beautiful Soup\\nfrom May 2006 to March 2012. It is considered stable, and only\\ncritical security bugs will be fixed. <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\'s\\nthe Beautiful Soup 3 documentation.</a>\\n</p>', '<a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\'s\\nthe Beautiful Soup 3 documentation.</a>', '<p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\\nthe same license as Python itself.\\n\\n</p>', '<p>The current release of Beautiful Soup 3 is <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\\n2012). You can install Beautiful Soup 3 with <code>pip install\\nBeautifulSoup</code>. It\\'s also available as\\n<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\\n<code>python-BeautifulSoup</code> in Fedora.\\n\\n</p>', '<a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a>', '<code>pip install\\nBeautifulSoup</code>', '<code>python-beautifulsoup</code>', '<code>python-BeautifulSoup</code>', '<p>You can also download the tarball and use\\n<code>BeautifulSoup.py</code> in your project directly.\\n\\n\\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\\n</p>', '<code>BeautifulSoup.py</code>', '<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>', '<h2>Hall of Fame</h2>', \"<p>Over the years, Beautiful Soup has been used in hundreds of\\ndifferent projects. There's no way I can list them all, but I want to\\nhighlight a few high-profile projects. Beautiful Soup isn't what makes\\nthese projects interesting, but it did make their completion easier:\\n\\n</p>\", '<ul>\\n<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n</li><li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n</li><li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n</li><li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n</li><li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>.\\n\\n</li><li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</li></ul>', '<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n</li>', '<a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>', '<li>Reddit uses Beautiful Soup to <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n</li>', '<a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>', '<li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n</li>', '<a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a>', '<li>The developers of Python itself used Beautiful Soup to <a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n</li>', '<a href=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>', '<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>.\\n\\n</li>', '<a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>', '<a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>', '<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</li>', '<a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a>', '<a href=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>', '<p>If you\\'ve used Beautiful Soup in a project you\\'d like me to know\\nabout, please do send email to me or <a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\ngroup</a>.\\n\\n</p>', '<a href=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\ngroup</a>', '<h2>Development</h2>', '<p>Development happens at <a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>.</p>', '<a href=\"https://launchpad.net/beautifulsoup\">Launchpad</a>', '<a href=\"https://code.launchpad.net/beautifulsoup/\">get the source\\ncode</a>', '<a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>', '<hr/>', '<table><tr><td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td><td valign=\"top\"><p><b>Document tree:</b>\\n</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>\\n\\n\\nSite Search:\\n\\n<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>\\n</td>\\n</tr>\\n</table>', '<tr><td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td><td valign=\"top\"><p><b>Document tree:</b>\\n</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>\\n\\n\\nSite Search:\\n\\n<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>\\n</td>\\n</tr>', '<td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p><p></p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></td>', '<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Tuesday, February 12 2019, 11:00:01 Nowhere Standard Time.</p>', '<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>', '<a href=\"/self/\">Leonard Richardson</a>', '<a href=\"/self/contact.html\">contact information</a>', '<p></p>', '<table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table>', '<tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td><td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr>', '<td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a></td>', '<a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a>', '<img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>', '<td valign=\"top\">Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td>', '<a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>', '<td valign=\"top\"><p><b>Document tree:</b>\\n</p><dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>\\n\\n\\nSite Search:\\n\\n<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>\\n</td>', '<p><b>Document tree:</b>\\n</p>', '<b>Document tree:</b>', '<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd></dl>', '<dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>\\n</dd>', '<a href=\"http://www.crummy.com/\">http://www.crummy.com/</a>', '<dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd></dl>', '<dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>\\n</dd>', '<a href=\"http://www.crummy.com/software/\">software/</a>', '<dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd></dl>', '<dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dd>', '<a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a>', '<form action=\"/search/\" method=\"get\">\\n<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\\n</form>', '<input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>']\n"
     ]
    }
   ],
   "source": [
    "## Question 8:\n",
    "html_words = [l for l in soup.findAll()]\n",
    "html_cwords = [\"\".join(str(a)) for a in html_words]\n",
    "print(html_cwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count\n",
      "if             5\n",
      "as             6\n",
      "for            6\n",
      "3              7\n",
      "projects       7\n",
      "can            7\n",
      "on             8\n",
      "or             8\n",
      "in            10\n",
      "it            10\n",
      "python        12\n",
      "is            12\n",
      "and           16\n",
      "a             16\n",
      "of            17\n",
      "you           20\n",
      "to            24\n",
      "the           33\n",
      "soup          35\n",
      "beautiful     35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([8., 4., 3., 2., 0., 3.]),\n",
       " array([ 5., 10., 15., 20., 25., 30., 35.]),\n",
       " <a list of 6 Patch objects>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Assign all text that appears in the body tag of the webpage into string variable browser_cwords as a single string.\n",
    "## Then I replace specific punctuations that would hinder the split process from working properly with a blank space.\n",
    "## Split the string afterwards by blank spaces and put into list `browser_tokens`\n",
    "browser_cwords = [l.get_text() for l in soup.findAll('body')]\n",
    "browser_cwords = browser_cwords[0].replace('\\n', ' ')\n",
    "browser_cwords = browser_cwords.replace('. ', ' ')\n",
    "browser_cwords = browser_cwords.replace('| ', ' ')\n",
    "browser_tokens = browser_cwords.split()\n",
    "#print(browser_cwords)\n",
    "\n",
    "## Reformat each string within list string browser_tokens to be lowcercase\n",
    "## Create new list with only the distinct words from the browser text.\n",
    "## Count and number of occurrences and put into dictionary matching with the names as keys\n",
    "browser_lctokens = [word.lower() for word in browser_tokens]\n",
    "browser_uniquelct = set(browser_lctokens)\n",
    "btoken_dict = {}\n",
    "for token in browser_uniquelct:\n",
    "    btoken_dict[token] = browser_lctokens.count(token)\n",
    "#print(btoken_dict)\n",
    "\n",
    "## Create dataframe where index is made of the key values from dictionary\n",
    "## Sort and get only the top 20 common words\n",
    "## Lastly, plot histogram with said info\n",
    "browser_df = pd.DataFrame.from_dict(btoken_dict,orient='index', columns = ['count'])\n",
    "browser_df = browser_df.sort_values(['count'])\n",
    "#print(browser_df)\n",
    "b_dftop20 = browser_df.tail(20)\n",
    "print(b_dftop20)\n",
    "plt.hist(b_dftop20['count'], bins=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
